{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conscious-incident",
   "metadata": {},
   "source": [
    "# Milestone 1: Tackling big data on your laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-copying",
   "metadata": {},
   "source": [
    "## Authors: Neel Phaterpekar, Arash Shamseddini and Charles Suresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cutting-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from memory_profiler import memory_usage\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.feather as feather\n",
    "import pyarrow.parquet as pq\n",
    "import dask.dataframe as dd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "closed-ceramic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n",
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conscious-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(dplyr)\n",
    "library(arrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-second",
   "metadata": {},
   "source": [
    "<Br><Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-bottom",
   "metadata": {},
   "source": [
    "## 1. Downloading the data\n",
    " \n",
    "In this section, we access rainfall data using the Figshare API. The following code directly grabs the data from Figshare and unzips the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-invalid",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-racing",
   "metadata": {},
   "source": [
    "### 1.1 Download data from figshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fresh-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = 14096681  # this is the unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"figsharerainfall/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thousand-detection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 318.33 MiB, increment: 0.13 MiB\n",
      "CPU times: user 84.3 ms, sys: 48.2 ms, total: 132 ms\n",
      "Wall time: 3.56 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'is_link_only': False,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'id': 26579150,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'size': 58863},\n",
       " {'is_link_only': False,\n",
       "  'name': 'environment.yml',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'id': 26579171,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'size': 192},\n",
       " {'is_link_only': False,\n",
       "  'name': 'README.md',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'id': 26586554,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'size': 5422},\n",
       " {'is_link_only': False,\n",
       "  'name': 'data.zip',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'id': 26766812,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'size': 814041183},\n",
       " {'is_link_only': False,\n",
       "  'name': 'get_data.py',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'id': 26766815,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'size': 4113}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "false-colon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.61 s, sys: 3.79 s, total: 8.4 s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"] # need only this zip file\n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True) # create the folder if not exists\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-booking",
   "metadata": {},
   "source": [
    "The downloaded data is now saved as 'data.zip'. The next step is to use ZipFile to extract the csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-soccer",
   "metadata": {},
   "source": [
    "### 1.2 Unzipping Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "democratic-archive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 321.38 MiB, increment: 0.00 MiB\n",
      "CPU times: user 15.8 s, sys: 2.28 s, total: 18.1 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-visitor",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-funeral",
   "metadata": {},
   "source": [
    "### 1.3 Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-third",
   "metadata": {},
   "source": [
    "\n",
    "| Contributors | Downloading wall time  | Extracting wall time  | \n",
    "|:---:|:-----:|:--------:|\n",
    "| Neel   |   1min 10s   |  23 s      | \n",
    "| Arash   | 10min 12s    | 26.6 s         |  \n",
    "| Charles   | 59.9s   |    19.9s      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-blake",
   "metadata": {},
   "source": [
    "> Suprisingly Arash's time usage for downloading the data.zip was significantly larger than those for Neel and Charles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-enforcement",
   "metadata": {},
   "source": [
    "## 2. Combining data CSVs\n",
    "\n",
    "In this section, we utilize the csv files that were attained from the figsharerainfall zip file. The code below concatenates all csv files into a new 'combined_csv' file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-zambia",
   "metadata": {},
   "source": [
    "There were 6 columns in each csv (with the exception of observed_daily_rainfall_SYD.csv). \n",
    "Since observed_daily_rainfall_SYD.csv was missing 4 columns, these columns were created and filled with NAs.\n",
    "\n",
    "The final data frame contains over 62 million rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-problem",
   "metadata": {},
   "source": [
    "### 2.1 Using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "confirmed-stretch",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reindex from a duplicate axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e64f58900537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mobs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lon_min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mobs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lon_max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mobs_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mobs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"figsharerainfall/observed_daily_rainfall_SYD.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/525/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOSITIONAL_OR_KEYWORD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/525/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4174\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4175\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4178\u001b[0m     def drop(\n",
      "\u001b[0;32m/opt/miniconda3/envs/525/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4810\u001b[0m         \u001b[0;31m# perform the reindex on the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4811\u001b[0;31m         return self._reindex_axes(\n\u001b[0m\u001b[1;32m   4812\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4813\u001b[0m         ).__finalize__(self, method=\"reindex\")\n",
      "\u001b[0;32m/opt/miniconda3/envs/525/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   4014\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4015\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4016\u001b[0;31m             frame = frame._reindex_columns(\n\u001b[0m\u001b[1;32m   4017\u001b[0m                 \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4018\u001b[0m             )\n",
      "\u001b[0;32m/opt/miniconda3/envs/525/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reindex_columns\u001b[0;34m(self, new_columns, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   4059\u001b[0m             \u001b[0mnew_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4060\u001b[0m         )\n\u001b[0;32m-> 4061\u001b[0;31m         return self._reindex_with_indexers(\n\u001b[0m\u001b[1;32m   4062\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4063\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/525/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   4875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4876\u001b[0m             \u001b[0;31m# TODO: speed up on homogeneous DataFrame objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4877\u001b[0;31m             new_data = new_data.reindex_indexer(\n\u001b[0m\u001b[1;32m   4878\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4879\u001b[0m                 \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/525/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;31m# some axes don't allow reindexing with dups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_reindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/525/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_can_reindex\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   3474\u001b[0m         \u001b[0;31m# trying to reindex on an axis with duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_as_unique\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3476\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot reindex from a duplicate axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reindex from a duplicate axis"
     ]
    }
   ],
   "source": [
    "col_names = pd.read_csv(\"figsharerainfall/ACCESS-CM2_daily_rainfall_NSW.csv\").columns.to_list()\n",
    "\n",
    "obs_df = pd.read_csv(\"figsharerainfall/observed_daily_rainfall_SYD.csv\")\n",
    "obs_df.insert(1, \"lat_min\", np.nan, True)\n",
    "obs_df.insert(2, \"lat_max\", np.nan, True)\n",
    "obs_df.insert(3, \"lon_min\", np.nan, True)\n",
    "obs_df.insert(4, \"lon_max\", np.nan, True)\n",
    "obs_df = obs_df.reindex(columns = col_names)\n",
    "\n",
    "obs_df.to_csv(\"figsharerainfall/observed_daily_rainfall_SYD.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dramatic-exhaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 473.93 MiB, increment: 0.00 MiB\n",
      "CPU times: user 6min 19s, sys: 24.9 s, total: 6min 44s\n",
      "Wall time: 6min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "\n",
    "files = glob.glob('figsharerainfall/*.csv')\n",
    "\n",
    "df = pd.concat((pd.read_csv(file, header=0, index_col=0)\n",
    "                .assign(model=re.search(r'/(.*)_d', file)[1])\n",
    "                for file in files))\n",
    "\n",
    "df.to_csv(\"figsharerainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "generic-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.9G\tfigsharerainfall/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh figsharerainfall/combined_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-victory",
   "metadata": {},
   "source": [
    "### 2.3 Summary table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-offering",
   "metadata": {},
   "source": [
    "| Contributors| Machine | Combining data wall time (pandas) | Memory |\n",
    "|:---:|:-----:|:-----:|:--------:|\n",
    "| Neel   |MacOS|   6min 51s   |  5.6G      | \n",
    "| Arash   |Windows|6min 10s    | 5.7G         |  \n",
    "| Charles   |MacOS| 5min 32s   |   6.6G       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-preview",
   "metadata": {},
   "source": [
    "### 2.2. Discussing observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-premises",
   "metadata": {},
   "source": [
    "The overall process of combining data was slow. Our group agreed that using pandas seemed to be the easiest method. Additionally some group members could not run the DASK on their machine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-introduction",
   "metadata": {},
   "source": [
    "> Both run times and memory usages on different machines within the team are pretty similar\n",
    "\n",
    "> Time usage: 5-7 min\n",
    "\n",
    "> Memory usage: 5.5-6.6G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-waterproof",
   "metadata": {},
   "source": [
    "## 3. Load the combined CSV to memory and performing a simple EDA\n",
    "\n",
    "In this section, the combined csv is loaded to memory. We attempt to reduce the memory usage of this large csv file through different methods (changing data dtype and using DASK). Finally, we explore the data from these new data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_float64 = pd.read_csv(\"figsharerainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-certification",
   "metadata": {},
   "source": [
    "### 3.1.1 Investigating Memory Reduction Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-cruise",
   "metadata": {},
   "source": [
    "#### Changing dtype of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_float32 = df_float64.astype('float32', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Memory usage with float64: {df_float64.memory_usage().sum() / 1e6:.2f} MB\")\n",
    "print(f\"Memory usage with float32: {df_float32.memory_usage().sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-minnesota",
   "metadata": {},
   "source": [
    "> Memory usage with float32 (2250.50 MB) is much lower than the memory usage with float64 (3500.78 MB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-sweden",
   "metadata": {},
   "source": [
    "####  Using Dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "ddf = dd.read_csv('figsharerainfall/combined_data.csv', blocksize=25e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']\n",
    "ddf[cols] = ddf[cols].astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-cruise",
   "metadata": {},
   "source": [
    "### 3.1.2 Python Simple EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-bristol",
   "metadata": {},
   "source": [
    "#### 3.1.2.1 Value Counts of `model`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-franchise",
   "metadata": {},
   "source": [
    "##### Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "print(df_float32[[\"model\"]].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-treatment",
   "metadata": {},
   "source": [
    "##### Dask DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "print(ddf[\"model\"].value_counts().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-melissa",
   "metadata": {},
   "source": [
    "##### Loading in Chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"figsharerainfall/combined_data.csv\", chunksize=10_000_000):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-visiting",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "\n",
    "|                  | Peak Memory | CPU Time | Wall Time |\n",
    "|:----------------:|:-----------:|:--------:|:---------:|\n",
    "| Pandas DataFrame |9312.58 MB  |  5.16s     |   5.16s        |\n",
    "|  Dask DataFrame  | 8822.06 MB |2min 12s     | 45.8          |\n",
    "|  Loading in Chunks| 6144.35 MB  |    1min 7s   |   1min 8s    |\n",
    "\n",
    "When calling `value_counts` function:\n",
    "- By using Dask DataFrame over Pandas DataFrame, we see that the Peak Memory drops by a considerable amount\n",
    "- However, the CPU and Wall Times are quite a bit higher when using Dask DataFrame over Pandas DataFrame\n",
    "- Loading the data in chunks as Pandas DataFrame, has, by far, the lowest Peak Memory usage. However, the wall time is the highest of all the three methods considered here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-adaptation",
   "metadata": {},
   "source": [
    "#### 3.1.2.2 Summary statistics of Rainfall (mm/day) across Australia:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-retirement",
   "metadata": {},
   "source": [
    "##### Pandas DataFrame `float64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "print(df_float64[['rain (mm/day)']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-motorcycle",
   "metadata": {},
   "source": [
    "##### Pandas DataFrame `float32`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "print(df_float32[['rain (mm/day)']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-discharge",
   "metadata": {},
   "source": [
    "##### Dask DataFrame `float32`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "print(ddf[['rain (mm/day)']].describe().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-office",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "\n",
    "|                            | Peak Memory | CPU Time | Wall Time |\n",
    "|:--------------------------:|:-----------:|:--------:|:---------:|\n",
    "| Pandas DataFrame `float64` |  7153.32 MB           | 4.98s         |  5.69s         |\n",
    "| Pandas DataFrame `float32` |  6199.48 MB           |   3.66s       | 4.38s          |\n",
    "|  Dask DataFrame `float32`  |   6412.95 MB          |   2min 20s      |  44.8s         |\n",
    "\n",
    "When calling the `describe` function:\n",
    "- The Peak Memory consumed is drops when using the Pandas DataFrame with `float32` numeric columns over the Pandas DataFrame with `float64` numeric columns\n",
    "- The CPU and Wall Times also drop when switching from the Pandas DataFrame with `float64` to the Pandas DataFrame with `float32` numeric columns\n",
    "- Interestingly, when compared with the Pandas DataFrame with `float32` numeric columns, the Dask DataFrame with `float32` numeric columns consumes more Peak Memory, and also has much higher CPU and Wall Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-airplane",
   "metadata": {},
   "source": [
    "#### 3.1.2.3 BoxPlot of Rainfall (mm/day) across Australia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (15, 7.5)}, font_scale=1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-hundred",
   "metadata": {},
   "source": [
    "##### Pandas DataFrame `float64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "sns.boxplot(x=df_float64['rain (mm/day)'], showfliers=False);\n",
    "plt.title(\"BoxPlot of Rainfall (mm/day) across Australia\");\n",
    "plt.xlabel(\"Rain (mm/day)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-lightning",
   "metadata": {},
   "source": [
    "##### Pandas DataFrame `float32`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "sns.boxplot(x=df_float32['rain (mm/day)'], showfliers=False);\n",
    "plt.title(\"BoxPlot of Rainfall (mm/day) across Australia\");\n",
    "plt.xlabel(\"Rain (mm/day)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-breeding",
   "metadata": {},
   "source": [
    "##### Dask DataFrame `float32`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "sns.boxplot(x=ddf['rain (mm/day)'], showfliers=False);\n",
    "plt.title(\"BoxPlot of Rainfall (mm/day) across Australia\");\n",
    "plt.xlabel(\"Rain (mm/day)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-melbourne",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "\n",
    "|                            | Peak Memory | CPU Time | Wall Time |\n",
    "|:--------------------------:|:-----------:|:--------:|:---------:|\n",
    "| Pandas DataFrame `float64` |  5824.61 MB           | 2.98s         |   3.66s        |\n",
    "| Pandas DataFrame `float32` |  5213.86 MB           |  2.28s        |   2.75s        |\n",
    "|  Dask DataFrame `float32`  |  6539.55 MB           |  6min 16s        |  2min 3s         |\n",
    "\n",
    "\n",
    "When creating boxplots using `sns.boxplot`:\n",
    "- The Peak Memory consumed is lower when using the Pandas DataFrame with `float32` numeric columns when compared with using the Pandas DataFrame with `float64` numeric columns\n",
    "- The CPU and Wall Times are also lower when using the Pandas DataFrame with `float32` numeric columns when compared with using the Pandas DataFrame with `float64` numeric columns\n",
    "- Interestingly, the Dask DataFrame with `float32` numeric columns consumes the most amount of Peak Memory and also has much higher CPU and Wall times when compared with the Panadas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-scott",
   "metadata": {},
   "source": [
    "## 4. R Approaches For Data Transfer\n",
    "\n",
    "In this section, we create different data structures to transfer the data that was previously in python to R. We use arrow tables, parquet files and feather files to explore different methods. We decided that the parquet file would be the optimal choice of method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-federal",
   "metadata": {},
   "source": [
    "### 4.1.1 Create Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-secretariat",
   "metadata": {},
   "source": [
    "#### Create table:\n",
    "\n",
    "This table can then be used to create the other files/data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "dataset = ds.dataset(\"figsharerainfall/combined_data.csv\", format=\"csv\")\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-proposition",
   "metadata": {},
   "source": [
    "#### Create Arrow Object:\n",
    "\n",
    "Here we are able to use python to create an arrow table that will then be passed into R. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "## Here we are loading the arrow dataframe that we have loaded previously\n",
    "arrow_table = pyra.converter.py2rpy(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-class",
   "metadata": {},
   "source": [
    "#### Create Feather File:\n",
    "\n",
    "Here we are creating a feather file that can then be loaded in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "feather.write_feather(table, 'figsharerainfall/combined_data.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "du -sh figsharerainfall/combined_data.feather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-overhead",
   "metadata": {},
   "source": [
    "#### Create Parquet File:\n",
    "\n",
    "Here we are creating a parquet file that can then be loaded in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "pq.write_table(table, 'figsharerainfall/combined_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "du -sh figsharerainfall/combined_data.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-amplifier",
   "metadata": {},
   "source": [
    "|                            | Disk Usage | Wall Time | \n",
    "|:--------------------------:|:-----------:|:---------:|\n",
    "| `combined_data.csv` |  6.6G          |      6min 51s   |\n",
    "| `combined_data.feather` |  1.0G          |    40.6s    | \n",
    "|  `combined_data.parquet`  |  544M        |     47.5s     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-drink",
   "metadata": {},
   "source": [
    "### 4.1.2 Transfer File Structure to R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-prevention",
   "metadata": {},
   "source": [
    "#### Using Arrow Object:\n",
    "\n",
    "Using the arrow table created above, we can easily transfer the data between python and R. However, this required us to use `pyra.converter.py2rpy()` in python to create the arrow table (shown above). This is an additional step that seems unnecesary; ideally we could just load the file directly into R instead of having to use both python and R to load in data.\n",
    "\n",
    "**Our opinion**: This method got the job done and it did not take very long relative to the other methods. However, this additional step of having to use python is inconvenient and can be avoided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R -i arrow_table\n",
    "start_time <- Sys.time()\n",
    "rview <- head(arrow_table)\n",
    "summary_df <- glimpse(arrow_table)\n",
    "model_count <- arrow_table %>% collect() %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(class(arrow_table))\n",
    "print(rview)\n",
    "print(summary_df)\n",
    "print(model_count)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-trial",
   "metadata": {},
   "source": [
    "#### Using Feather File:\n",
    "Here we can directly read in the feather file that was created above \n",
    "\n",
    "**Our opinion:** This method was a pretty fast way to load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "### her we are showing how much time it took to read a feather file what we wrote in python\n",
    "\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_feather(\"figsharerainfall/combined_data.feather\")\n",
    "rview <- head(r_table)\n",
    "summary_df <- glimpse(r_table)\n",
    "model_count <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(class(r_table))\n",
    "print(rview)\n",
    "print(summary_df)\n",
    "print(model_count)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-possession",
   "metadata": {},
   "source": [
    "#### Using Parquet File:\n",
    "\n",
    "Similarly to the previous section, we directly load a file to R. Here we use a parquet file to load in the data. \n",
    "\n",
    "**Our opinion:** This will likely be our method of choice. The memory usage of the parquet is the smallest of the 3 methods and the over speed of this method is was also quite fast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_parquet(\"figsharerainfall/combined_data.parquet\")\n",
    "view <- head(r_table)\n",
    "summary_df <- glimpse(r_table)\n",
    "model_count <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(class(r_table))\n",
    "print(view)\n",
    "print(summary_df)\n",
    "print(model_count)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-notice",
   "metadata": {},
   "source": [
    "\n",
    "|     Charles                       | End Time - Start Time | CPU Time | Wall Time |\n",
    "|:--------------------------:|:-----------:|:--------:|:---------:|\n",
    "| Using Arrow Object |  7.617s          | 20.1s         |   7.79s        |\n",
    "| Using Feather File |  22.245s           |  44.5s        |   23.1s        |\n",
    "|  Using Parquet File  |  12.161s         |  20.3s        |  13.4s         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-applicant",
   "metadata": {},
   "source": [
    "|     Neel                      | End Time - Start Time | CPU Time | Wall Time |\n",
    "|:--------------------------:|:-----------:|:--------:|:---------:|\n",
    "| Using Arrow Object |  34.69s          | 15.2s         |   35.3s        |\n",
    "| Using Feather File |  74.104s           |  25.1s        |   77s        |\n",
    "|  Using Parquet File  |  56.478s         |  28.1s        |  59.4s         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-investigation",
   "metadata": {},
   "source": [
    "### 4.2 Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-space",
   "metadata": {},
   "source": [
    "After comparing these 3 methods:\n",
    "\n",
    "- We can see that is quite a bit of variability between the 3 methods in terms of time. \n",
    "\n",
    "- All three of these methods are much more efficient that using csvs in terms of memory and speed. \n",
    "\n",
    "- It should be noted that rpy2-arrow is still fairly new. It is possible that in the future this method of conversion can improve and become more popular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-rolling",
   "metadata": {},
   "source": [
    "We will pick parquet file format to transfer data from python to R. The reasons are listed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-revolution",
   "metadata": {},
   "source": [
    "- The speed with which the file was created was quite fast.\n",
    "- The memory that the file takes is significantly less than the feather file and arrow table. \n",
    "- The time that it takes to transfer the data to R is reasonable (comparable to the other methods)\n",
    "- The columnar properties of the parquet file would make it quite easy to use in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-participation",
   "metadata": {},
   "source": [
    "## 5. Issues "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-fiction",
   "metadata": {},
   "source": [
    "Overall, it was pretty frustrating to have to run the entire notebook several times. Working with big data is a slow process and running individual cells that take multiple minutes at a time can be arduous. However, there really is nothing that can be done about that other than trying to use the most efficient data storage methods (which we have done here)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-consideration",
   "metadata": {},
   "source": [
    "\n",
    "Neel:\n",
    "- Neel had issues with maximum memory. Neel's computer actually requested that he force quit some applications because possible memory usage was being exceeded. We were unsure what exactly caused this. Neel ended up force quitting everything and restarting Jupyter. This got rid of the problem \n",
    "\n",
    "- Neel had quite a few issues with loading in the arrow table into the R cell magic. On the first day, nothing was happening, even after 20 minutes of waiting for the data to load. Again the solution here was to restart applications and try again on a different day. \n",
    "\n",
    "- The regex that used to grab the model names from the individual csv files was not working on my computer, although it was working on Arash's. Also not sure why this happened. \n",
    "\n",
    "Charles:\n",
    "- I mainly faced issues regarding the time need to run certain code chunks: I initially tried to plot a time-series plot for rainfall across Australia. However, even after an hour the code chunk for the time-series plot kept running. So, I had to settle for simple Boxplots\n",
    "- My computer would randomly freeze for extended periods of time while running certain code chunks. To prevent the computer from freezing, I had to close all other background processes and apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-milan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
